\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage[super]{nth}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{hyperref}

\pagenumbering{arabic}

\begin{document}

\title{Graphical Analysis in ArtToMusic}
\date{December 06, 2016}
\author{Rafael De Smet}

\maketitle
\tableofcontents
\newpage
\section{Introduction}

In this paper I will be discussing several graphical analysis algorithms. These algorithms will return data which are used to base the music generation on. Not all algorithms are used in the code, but are discussed for the sake of completeness.

\section{Algorithms}

\subsection{Edge Detection}

The concept of edge detection is pretty straightforward. The algorithms try to detect all the edges in an image. Edges are the contours of objects and shapes in the image.
Edge detection algorithms all use what are called convolution kernels. A kernel in image processing is a small matrix used to apply effects to an image, such as blurring and outlining. Here we will see kernels used for edge detection only. Listed below are six of the best and most used algorithms.

\begin{itemize}
        \item Sobel 
        \item Frei-Chen
        \item Prewitt
        \item Roberts Cross
        \item LoG
        \item Scharr
\end{itemize}

\subsubsection{Convolution kernel}

Convolution is the technique of multiplying together two arrays of different size but of the same dimension. An array of dimension two simply is a matrix. When working with images the pixels are represented as a (2D) matrix and the kernel is also a 2D matrix.
The kernel is a small matrix that we will multiply with the image matrix to perform the convolution. This kernel matrix is different for each edge detection algorithm. We will see examples of different matrices later in this paper. 
\newline

The idea of these algorithms is to create a new image that shows the edges of the original image. Each pixel of the original image is added to its local neighbours, weighted by the kernel. This produces a new image. The result of the matrix multiplication results in new pixel values that denote the edges of the image. 
Mathematically we can write the convolution as follows, with $O$ the output image, $I$ the input image and $K$ the kernel. $I(i,j)$ means the pixel on the $i^{th}$ row and the $j^{th}$ column.

\begin{equation}
O(i, j) =  \sum\limits_{k=1}^m\sum\limits_{l=1}^n I(i + k - 1, j + l - 1)K(k,l)
\end{equation}
\newline

An example will clarify the previous.

\begin{figure}[h]
\centering
\includegraphics[scale = 0.5]{img/convolution}
\caption{A pixel matrix and a kernel matrix}
\end{figure}

Using the kernel matrix we will compute every new pixel of the output image, by sliding the kernel matrix over the original pixels. Each kernel position corresponds to a single output pixel, the value of which is calculated by equation (1).
\newline

In our example, the value of the bottom right pixel in the output image will be found as follows [2]:

\begin{equation}
O_{57} = I_{57}K_{11} + I_{58}K_{12} + I_{59}K_{13} + I_{67}K_{21} + I_{68}K_{22} + I_{69}K_{23}
\end{equation}
\newpage

In figure 2 you can see a painting by Picasso. We will use this image to see how well the different edge detection algorithms work.

\begin{figure}[h]
\centering
\includegraphics[scale = 0.39]{img/picasso}
\caption{Picasso}
\end{figure}

\subsubsection{Sobel}

 The Sobel algorithm performs a 2D spatial gradient measurement and defines regions of 'high spatial frequency' or edges. It uses two 3x3 kernels, one for the horizontal edges and one for the vertical edges. These two kernels are applied consecutively and the results are combined to define all the edges. In figure 3 you can see the result of the Sobel filter.
 \newline

 The horizontal kernel: 
 $\begin{vmatrix}
        -1 & 0 & +1\\
        -2 & 0 & +2\\
        -1 & 0 & +1\\
\end{vmatrix}$
and the vertical kernel:
$\begin{vmatrix}
        +1 & +2 & +1\\
        0 & 0 & 0\\
        -1 & -2 & -1\\
\end{vmatrix}$

\begin{figure}[h]
\centering
\includegraphics[scale = 0.39]{img/picassoSobel}
\caption{Sobel filter applied to Figure 2}
\end{figure}
\newpage

\subsubsection{Frei-Chen}
 The Frei-Chen algorithm also uses 3x3 kernels, but this time there are nine different convolution kernels. The four first matrices, G1, G2, G3, G4, are used for edges, the next four are used for lines and $G_9$ is used to compute averages. $G_9$ attenuates the impact of the computations from the other matrices. Figure 4 shows the result of the Frei-Chen filter.
\newline
\newline
 $G_1$ = $\frac{1}{2\sqrt2}$ $\begin{vmatrix}
        1 & \sqrt2 & 1\\
        0 & 0 & 0\\
        -1 & -\sqrt2 & -1\\
\end{vmatrix}$\hspace{5mm}
$G_2$ = $\frac{1}{2\sqrt2}$$\begin{vmatrix}
        1 & 0 & -1\\
        \sqrt2 & 0 & -\sqrt2\\
        1 & 0 & -1\\
\end{vmatrix}$\hspace{5mm}
$G_3$ = $\frac{1}{2\sqrt2}$$\begin{vmatrix}
        0 & -1 & \sqrt2\\
        1 & 0 & -1\\
        -\sqrt2 & 1 & 0\\
\end{vmatrix}$\hspace{5mm}
\newline
$G_4$ = $\frac{1}{2\sqrt2}$$\begin{vmatrix}
        \sqrt2 & -1 & 0\\
        -1 & 0 & 1\\
        0 & 1 & -\sqrt2\\
\end{vmatrix}$\hspace{5mm}
$G_5$ = $\frac{1}{2}$$\begin{vmatrix}
        0 & 1 & 0\\
        -1 & 0 & -1\\
        0 & 1 & 0\\
\end{vmatrix}$\hspace{10mm}
$G_6$ = $\frac{1}{2}$$\begin{vmatrix}
        -1 & 0 & 1\\
        0 & 0 & 0\\
        1 & 0 & -1\\
\end{vmatrix}$\hspace{5mm}
\newline
$G_7$ = $\frac{1}{6}$$\begin{vmatrix}
        1 & -2 & 1\\
        -2 & 4 & -2\\
        1 & -2 & 1\\
\end{vmatrix}$\hspace{13mm}
$G_8$ = $\frac{1}{6}$$\begin{vmatrix}
        -2 & 1 & -2\\
        1 & 4 & 1\\
        -2 & 1 & -2\\
\end{vmatrix}$\hspace{11mm}
$G_9$ = $\frac{1}{3}$$\begin{vmatrix}
        1 & 1 & 1\\
        1 & 1 & 1\\
        1 & 1 & 1\\
\end{vmatrix}$\hspace{5mm}

\begin{figure}[h]
\centering
\includegraphics[scale = 0.39]{img/picassoFreiChen}
\caption{Frei-Chen filter applied to Figure 2}
\end{figure}

\subsubsection{Prewitt}
This algorithm is very similar to the Sobel algorithm. Again, two kernels are used, one for the horizontal and one for the vertical edges. In this case the kernels are basic convolution filters of the following form. 
\newline

Horizontal filter = $\begin{vmatrix}
        1 & 1 & 1\\
        0 & 0 & 0\\
        -1 & -1 & -1\\
\end{vmatrix}$\hspace{11mm}
Vertical filter = $\begin{vmatrix}
        -1 & 0 & 1\\
        -1 & 0 & 1\\
        -1 & 0 & 1\\
\end{vmatrix}$
\newline

\begin{figure}[h]
\centering
\includegraphics[scale = 0.39]{img/picassoPrewitt}
\caption{Prewitt filter applied to Figure 2}
\end{figure}
\newpage

\subsubsection{Roberts Cross}
This algorithm uses even simpler kernels than Prewitt does. This time we use two 2x2 kernels. These kernels correspond to the edges running at 45Â° to the pixel grid, one for each of the two perpendicular orientations. Figure 6 shows the result of the Roberts Cross filter.
\newline

Horizontal filter = $\begin{vmatrix}
        1 & 0 \\
        0 & -1 \\
\end{vmatrix}$\hspace{11mm}
Vertical filter = $\begin{vmatrix}
        0 & 1\\
        -1 & 0\\
\end{vmatrix}$

\begin{figure}[h]
\centering
\includegraphics[scale = 0.39]{img/picassoRobertsCross}
\caption{Roberts Cross filter applied to Figure 2}
\end{figure}

\subsubsection{LoG}
This algorithm first applies Gaussian filtering\footnote{The Gaussian filter is used to blur images and remove noise and detail.} to the image and then the Laplacian method for edge detection\footnote{The Laplacian method highlights regions in the image of rapid intensity change, so it is useful for edge detection.}, hence the name "Laplacian of Gaussian" (LoG).
The edge points of an image are detected by finding the zero crossings of the \nth{2} derivative of the image intensity. Because the \nth{2} derivative is very sensitive to noise, which could give us bad results, the Gaussian filter is used to clear the noise from the image. In figure 7, you can find the result of this filter.
\newline

The R library OpenImageR\footnote{https://cran.r-project.org/web/packages/OpenImageR/OpenImageR.pdf} uses the following LoG mask.
\newline

LoG mask = $\begin{vmatrix}
        1 & 1 & 1\\
        1 & -8 & 1\\
        1 & 1 & 1\\
\end{vmatrix}$

\begin{figure}[h]
\centering
\includegraphics[scale = 0.39]{img/picassoLoG}
\caption{LoG filter applied to Figure 2}
\end{figure}

\subsubsection{Scharr}
The last algorithm is again an extension of the Sobel algorithm. This operator improves rotational invariance, which means that if the image is rotated, the operator should define the same edges. Sobel can have difficulties with this aspect. There are many Scharr kernels, some even 5x5, but the following are most frequently used. Figure 8 shows the results.
\newline

Horizontal filter = $\begin{vmatrix}
        3 & 10 & 3\\
        0 & 0 & 0\\
        -3 & -10 & -3\\
\end{vmatrix}$\hspace{11mm}
Vertical filter = $\begin{vmatrix}
        3 & 0 & -3\\
        10 & 0 & -10\\
        3 & 0 & -3\\
\end{vmatrix}$\hspace{5mm}

\begin{figure}[h]
\centering
\includegraphics[scale = 0.39]{img/picassoScharr}
\caption{Scharr filter applied to Figure 2}
\end{figure}
\newpage

\subsection{Color Analysis}
Only edge detection algorithms are not sufficient to get enough data from the image to form a musical interpretation. We will need some form of color analysis as well. This section describes some algorithms and methods to obtain information about the colors of the image.

\subsubsection{RGB - HSV - CMYK}
To represent color on the computer the RGB representation is the most commonly used. Each pixel is described using three values, the amount of red (R), green (G) and blue (B). Using these values we can count how many pixels in the image are dominantly red, green or blue. When we're going to translate the image data into musical patterns, we can match each color to another kind of music (happy, sad, etc...)
\newline

HSV, sometimes called HSB, is another representation of a pixel. This time we use the hue (color, denoted by H), the saturation (S) and the brightness or value (B or V) to describe the pixel. Where the RGB model consists of three values indicating the amounts of a certain color each pixel has, the HSV model consists of three independent components to form a color. 
\newline

\paragraph{Hue} First we have the hue, which is the color. The color is represented using a circle with all the colors on it. The value of this component is the degrees of the angle we have to make on the circle to get this color.  

\paragraph{Saturation} The saturation indicates the fullness of the color. This value is expressed in a percentage, with 0\% a gray, flat color and 100\%  a full, rich color.

\paragraph {Value of Brightness} The value or brightness indicates the amount of light of the color and is also expressed in a percentage, 0\% is black and 100\% is white.

\begin{figure}[h]
\centering
\includegraphics[scale = 0.45]{img/hsv}
\caption{The HSV color representation}
\end{figure}

CMYK is another representation of color, based on the mixing of four colors, cyan (C), magenta (M), yellow (Y) and key (K, black). This model is used frequently with printing, but not very interesting for this paper.

\subsubsection{Black And White Balance}
During the image analysis we will convert the image to a gray scale image, so we can determine the amount of white and black in it. Using the RGB model, this is an easy process. Black in the RGB model is (0,0,0) and white is (255,255,255). So we can simply count the amount of pixels that are very close to those two values and we know the amount of black and white pixels in the image.
Analogous to this we can count every gray pixel in the image, by finding every pixel where the RGB values are exactly the same.

\subsubsection{Color Quantization (Image Segmentation)}

Sometimes it can be easier to work with images that are partitioned in simpler regions. This technique is called color quantification or image segmentation. There are many different ways to implement this technique. One such approach is using the k-means cluster algorithm. This algorithm attempts to partition a data set into $k$ clusters. The data set contains the RGB values of each pixel.
The disadvantage of this algorithm is that we don't know what the right value for $k$ is. So we have to perform this algorithm a couple of times, each time with a different k $k$ to see which value gives us a good result.
\newline

A second technique is the X-means algorithm which is an improvement of the k-means algorithm. The X-means algorithm can determine the number of clusters by itself, using the Bayesian information criterion (BIC). BIC is a selection criterion for clustermodels. Based on the total number of clusters found by X-means, we can generate parts of the piece of music.

\subsection{Image Hashing}
It is also possible to get the hash value of an image. The hash is a hexadecimal number and is another representation of the image. This is used to compare images. 

\subsubsection{Average Hashing}
The first hash method is the average hash of the image. This algorithm contains five steps.

\begin{enumerate}
        \item Convert the image to grayscale. 
        \item Reduce the size of the image, to reduce the number of computations.
        \item Average the resulting colors. For an 8x8 image, 64 will be averaged.
        \item Compute the bits of the hash value by comparing if each color value is above or below the mean.
        \item Construct the hash.
\end{enumerate}

\subsubsection{Difference Hash}
We can also compute the dHash of an image. It is similar to the average hash, but now the difference between adjacent pixels is also considered in the computation.

\subsubsection{Perceptive Hash}
This method uses the discrete cosine transform (DCT) and compares the pixels based on the frequencies, given by the DCT, instead of the color values.
DCT is a similar technique as Fourier analysis, it expresses a finite sequence of points, in this case the pixels, in terms of a sum of cosine functions.

\subsubsection{Use Of The Hash}
One application of these methods is the recognition of images. Search engines like Google use this technique to search similar images to any image you provide to the search engine. I could use this as a way to store information about the image my application has already scanned and generated music for.
\newline

For example, if you use a picture of a tree with the ArtToMusic application, it will produce a certain kind of music. If you use another picture of another tree that looks quite similar to the first tree, you expect to get a similar result in the music. The hash values of the images can help with this. 

\subsection{Entropy}

Still another method we can use to get information from an image is to calculate the entropy.
The entropy is a measure of the amount of disorder in the image.
\newline

When talking about images, we consider the following. If all the pixels of the image have the same level of a specific parameter, the entropy is zero. This means there is not a lot of information (almost none) to be gained from the image. An image of a blue sky has a very low entropy cause we only see blue. 
\newpage

When all the pixels in the image are different, the entropy of the image is maximum and there is a lot of information to get from the image. As the entropy describes how much (dis)order there is, we will use this to determine how much (dis)order there is in the generated music.
\newline

To get the right information about the pixels, we use a histogram that shows the count of the distinct pixel values. The entropy of an image $H$ is defined as:
\begin{equation}
H = - \sum_{k=0}^{M-1} p_k log_2 (p_k)
\end{equation}

where M is the number of unique pixel values and $p_k$ is the count of each pixel level, how many times this pixel level occures in the image.
\newline

In the ArtToMusic application we will investigate whether it is useful to use the entropy of the original image in the music generation process. We can as well calculate the entropy of the result of a specific analysis and we expect that this approach will be more beneficial.
  
\begin{thebibliography}{1}

\bibitem{OpenImageR} Lampros Mouselimis {\em OpenPackageR, An Image Processing Toolkit} 2017

\bibitem{Convolution} R. Fisher, S. Perkins, A. Walker, E. Wolfart {\em Hypermedia Image Processing Reference} 2003

\bibitem{Frei_Chen_Sobel} Daniel R\'akos' blog (\url{http://rastergrid.com/blog/2011/01/frei-chen-edge-detector/}) 2011

\bibitem{Gaussian} University of Auckland, New Zealand, Computer Science course Computer Graphics and Image Processing {\em Gaussian Filtering} 2010

\bibitem{edge} ComputaÃ§\~ao Visual e MultimÃ©dia, course on Image Processing

\bibitem{prewitt} RoboRealm blog (\url{http://www.roborealm.com/help/Prewitt.php}) 2017

\bibitem{scharr} Jogn Costella's blog ( \url{http://johncostella.com/edgedetect/}) 2012

\bibitem{image filter} Simon Colton, Pedro Torres {\em Evolving Approximate Image Filters} 2009, Computational Creativity Group
Department of Computing, Imperial College London

\bibitem{hash} Dr. Neal Krawetz' blog (\url{http://www.hackerfactor.com/blog/?/archives/529-Kind-of-Like-That.html}) 2013

\bibitem{entropy} John Loomis' site (\url{http://www.johnloomis.org/ece563/notes/basics/entropy/entropy.html}) 1998

\bibitem{entropy} Kevin Meurer's blog (\url{http://kevinmeurer.com/a-simple-guide-to-entropy-based-discretization/}) 2015

\bibitem{color} Ryan Walker's blogpost (\url{https://www.r-bloggers.com/color-quantization-in-r/}) 2016

\end{thebibliography}

\end{document}
